# -*- coding: utf-8 -*-
"""Data Hack Loan Risk Management.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18nSS9RKYb0JFVDmuRBR3yR4DVggTo1Z4
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pandas import Series,DataFrame 
from datetime import date
import datetime as DT
import io
from scipy import stats

from mpl_toolkits.mplot3d import Axes3D
import matplotlib as mpl
# %matplotlib inline



# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive')

train_set=pd.read_csv("/content/drive/My Drive/data hack datasets/Datahack/Data_Hack_train_values.csv")
train_labels=pd.read_csv("/content/drive/My Drive/data hack datasets/Datahack/Data_Hack_train_labels.csv")
test_set=pd.read_csv("/content/drive/My Drive/data hack datasets/Datahack/Data_Hack_test_values.csv")

plt.hist(train_set.loan_amount, bins=10)
plt.show()

sns.set(style="ticks")

#train_set = sns.load_dataset("train_set")
sns.pairplot(train_set)

#dropping the row_id in train_labels in order to avoid duplication as it already exists in the train_set
train_labels=train_labels.drop(['row_id'], axis=1)

train_set=pd.concat([train_set,train_labels], axis=1)

#Creating and storing our target variable in a seperate dataframe
Accepted=DataFrame(train_set["accepted"])

#Here we want to merge our training and test set but for the columns to be same, we must first take of our target in train
train_set=train_set.drop(['accepted'], axis=1)

#Now we merge BUT first we must create 2 train columns in our test and train set in order to id and seperate them later
train_set['train']=1
test_set['train']=0

combined=pd.concat([train_set, test_set])

#filling some missing vales with MEAN AND MODE as case may be-----TEST SET
cols_fill_missing = ["applicant_income", "population","minority_population_pct","ffiecmedian_family_income","tract_to_msa_md_income_pct","number_of_owner-occupied_units","number_of_1_to_4_family_units"]
combined[cols_fill_missing]=combined[cols_fill_missing].fillna(combined.mean().iloc[0])

#combined[300:600]

# Histogram
fig = plt.figure(figsize = (6,4))
title = fig.suptitle("Histogram for Applicant Income", fontsize=14)
fig.subplots_adjust(top=0.85, wspace=0.3)

ax = fig.add_subplot(1,1, 1)
ax.set_xlabel("Applicant income")
ax.set_ylabel("Frequency") 
ax.text(1.2, 800, r'$\mu$='+str(round(combined['applicant_income'].mean(),2)), 
         fontsize=12)
freq, bins, patches = ax.hist(combined['applicant_income'], color='steelblue', bins=15,
                                    edgecolor='black', linewidth=1)
                                    

# Density Plot
fig = plt.figure(figsize = (6, 4))
title = fig.suptitle("KDE plot Applicant income", fontsize=14)
fig.subplots_adjust(top=0.85, wspace=0.3)

ax1 = fig.add_subplot(1,1, 1)
ax1.set_xlabel("Applicant Income")
ax1.set_ylabel("Frequency") 
sns.kdeplot(combined['applicant_income'], ax=ax1, shade=True, color='steelblue')



plt.matshow(train_set.corr())
plt.show()

corr=train_set.corr()
corr.style.background_gradient(cmap='coolwarm')

train_set.mean().plot(style='o')

train_set.median().plot(style='o')

#but when we shuffle our columns , we have

train_set.mean().sort_values().plot(style='.')

#Encoding categorical variables
combined[200:500]

#cleaning missing values i.e replacing missing values with nan
#CONVERTS NEGATIVE NUMBERS TO nan:
combined["msa_md"].loc[combined["msa_md"]<0]=182.0

combined["county_code"].loc[combined["county_code"]<0]=combined["county_code"].median()

combined["state_code"].loc[combined["state_code"]<0]=37



combined["lender"].median()
combined["lender"].loc[combined["lender"]<0]=3718.0

combined["loan_amount"].mean()
combined["loan_amount"].loc[combined["loan_amount"]<0]=222.0

combined["loan_type"].mode()
combined["loan_type"].loc[combined["loan_type"]<0]=1

combined["property_type"].mode()
combined["property_type"].loc[combined["property_type"]<0]=1

combined["loan_purpose"].mode()
combined["loan_purpose"].loc[combined["loan_purpose"]<0]=3

combined["occupancy"].mode()
combined["occupancy"].loc[combined["occupancy"]<0]=1

combined["preapproval"].mode()
combined["preapproval"].loc[combined["preapproval"]<0]=3

combined["applicant_income"].median()
combined["applicant_income"].loc[combined["applicant_income"]<0]=79
combined["applicant_income"].loc[combined["applicant_income"]>240000]=79

combined["population"].median()
combined["population"].loc[combined["population"]<0]=5099
combined["population"].loc[combined["population"]>240000]=5099

combined["minority_population_pct"].median()
combined["minority_population_pct"].loc[combined["minority_population_pct"]<0]=25
combined["minority_population_pct"].loc[combined["minority_population_pct"]>240000]=25

combined["ffiecmedian_family_income"].median()
combined["ffiecmedian_family_income"].loc[combined["ffiecmedian_family_income"]<0]=68648
combined["ffiecmedian_family_income"].loc[combined["ffiecmedian_family_income"]>240000]=68648

combined["tract_to_msa_md_income_pct"].median()
combined["tract_to_msa_md_income_pct"].loc[combined["tract_to_msa_md_income_pct"]<0]=100
combined["tract_to_msa_md_income_pct"].loc[combined["tract_to_msa_md_income_pct"]>240000]=100

combined["number_of_owner-occupied_units"].median()
combined["number_of_owner-occupied_units"].loc[combined["number_of_owner-occupied_units"]<0]=1364
combined["number_of_owner-occupied_units"].loc[combined["number_of_owner-occupied_units"]>240000]=1364

combined["number_of_1_to_4_family_units"].median()
combined["number_of_1_to_4_family_units"].loc[combined["number_of_1_to_4_family_units"]<0]=1797
combined["number_of_1_to_4_family_units"].loc[combined["number_of_1_to_4_family_units"]>240000]=1797

combined["applicant_ethnicity"].mode()
combined["applicant_ethnicity"].loc[combined["applicant_ethnicity"]<0]=2
combined["applicant_ethnicity"].loc[combined["applicant_ethnicity"]>240000]=2

combined["applicant_race"].median()
combined["applicant_race"].loc[combined["applicant_race"]<0]=5
combined["applicant_race"].loc[combined["applicant_race"]>240000]=5

combined["applicant_sex"].median()
combined["applicant_sex"].loc[combined["applicant_sex"]<0]=1
combined["applicant_sex"].loc[combined["applicant_sex"]>240000]=1

#checking if there is any missing value in our dataframe
combined.isnull().values.any()
combined.isnull().any()

len(combined['applicant_sex'].unique())

combined.info()

#ENCODINGS GET DUMMIES STARTS HERE

#NOW WE ENCODE
combined1=pd.get_dummies(combined['msa_md'], drop_first=True)
#now we encode
combined2=pd.get_dummies(combined['state_code'], drop_first=True)
#now we encode
combined3=pd.get_dummies(combined['county_code'], drop_first=True)


combined4=pd.get_dummies(combined['loan_type'], drop_first=True)
#now we encode
combined5=pd.get_dummies(combined['property_type'], drop_first=True)
#now we encode
combined6=pd.get_dummies(combined['loan_purpose'], drop_first=True)


combined7=pd.get_dummies(combined['occupancy'], drop_first=True)
#now we encode
combined8=pd.get_dummies(combined['preapproval'], drop_first=True)
#now we encode
combined9=pd.get_dummies(combined['applicant_ethnicity'], drop_first=True)

combined10=pd.get_dummies(combined['applicant_race'], drop_first=True)
#now we encode
combined11=pd.get_dummies(combined['applicant_sex'], drop_first=True)
#now we encode
combined12=pd.get_dummies(combined['co_applicant'], drop_first=True)

combined.shape

#now we combine all datasets together
combined=pd.concat([combined,combined1,combined2,combined3,combined4,combined5,combined6,combined7,combined8,combined9,combined10,combined11,combined12], axis=1)

#i am now dropping the string/object/boolean categorical features cos they have been converted to dummies
combined=combined.drop([ 'msa_md','state_code','county_code', 'loan_type', 'property_type', 'loan_purpose', 'occupancy', 'preapproval','applicant_ethnicity', 'applicant_race', 'applicant_sex','co_applicant','row_id'], axis=1)

combined.shape

#Now its time to separate those two datasets and we are done with both the train and test set now 
#containing the same number of columns.
train_df=combined[combined['train']==1]
test_df=combined[combined['train']==0]
#we now drop the train columns we created in test and train set
train_df.drop(['train'], axis=1, inplace=True)
test_df.drop(['train'], axis=1, inplace=True)

#now we combine back our target to our train set together
train_df=pd.concat([train_df,Accepted], axis=1)

# Predicting the Test set results and converting to csv
prediction = pd.DataFrame(train_df).to_csv('prediction42.csv')

# Predicting the Test set results and converting to csv
prediction = pd.DataFrame(test_df).to_csv('datahack_test.csv')

train_df.shape

test_df.shape

#Splitting  the dataset into X and y
X = train_df.iloc[:, 0:814].values
y = train_df.iloc[:, -1].values

# Splitting the dataset into the Training set and validation set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

#Here we are generating our synthetic points to balance the scarce class using the SMOTE method
#from imblearn.over_sampling import SMOTE
#sm=SMOTE(random_state=11, ratio=1.0)
#X_train, y_train = sm.fit_sample(X_train, y_train)

#Then we shuffle cos the new rows are mostimes jampacked together
#from sklearn.utils import shuffle
#X_train,y_train= shuffle(X_train,y_train)

#we statndard scale(note as fi score and presicion increased as soon as i standard scalled )
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
test_df = sc_X.transform(test_df)

from sklearn.datasets import make_classification
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix

